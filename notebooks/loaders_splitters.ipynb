{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347285bd",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a41fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cba7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"stock_news.txt\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8353079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RBC Capital upgraded shares of GoDaddy (NYSE:GDDY) to Outperform from Sector Perform in a note Wednesday, raising the price target to $124 per share, up from $90.\\n\\nAnalysts told investors that the company is \"structurally differentiated\" and under-owned.\\n\\n\"GDDY\\'s an under-owned, durably growing cash machine with a dash of AI,\" wrote the analysts. \"We like its structurally hedged customer acquisition model, have rising confidence in margin expansion coming out of our conference and lastly, like management\\'s openness to increasingly managing top-line expectations.\"\\n\\nThe analysts also noted that the company\\'s generative AI-focused dinner next week is timely, while the mid-Q1 analyst day should improve visibility into the company\\'s strong long-term earnings and free cash flow generation.\\n\\n\"The stock could be viewed as trading at mid-single digit multiple on very reachable longer-term assumptions,\" added the analysts.', metadata={'source': 'stock_news.txt'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f064833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6b9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\"C:\\\\Users\\\\priya\\\\Downloads\\\\Bengaluru_House_Data.csv\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634a6fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13320"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee163da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='area_type: Super built-up  Area\\navailability: 19-Dec\\nlocation: Electronic City Phase II\\nsize: 2 BHK\\nsociety: Coomee\\ntotal_sqft: 1056\\nbath: 2\\nbalcony: 1\\nprice: 39.07', metadata={'source': 'C:\\\\Users\\\\priya\\\\Downloads\\\\Bengaluru_House_Data.csv', 'row': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a76414e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315c2c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:\\\\Users\\\\priya\\\\Downloads\\\\Bengaluru_House_Data.csv', 'row': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba4a8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\"C:\\\\Users\\\\priya\\\\Downloads\\\\Bengaluru_House_Data.csv\", source_column=\"location\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "638f9962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Electronic City Phase II', 'row': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b108a3",
   "metadata": {},
   "source": [
    "# Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d770c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Picture this – you’re working on a really cool data science project and have applied the latest state-of-the-art library to get a pretty good result. And boom! A few days later, there’s a new state-of-the-art framework in town that has the potential to further improve your model.\n",
    "\n",
    "That is not a hypothetical scenario – it’s the reality (and thrill) of working in the field of Natural Language Processing (NLP)! The last two years have been mind-blowing in terms of breakthroughs. I get to grips with one framework and another one, potentially even better, comes along.\n",
    "\n",
    "Google’s BERT is one such NLP framework. I’d stick my neck out and say it’s perhaps the most influential one in recent times (and we’ll see why pretty soon).\n",
    "\n",
    "It’s not an exaggeration to say that BERT has significantly altered the NLP landscape. Imagine using a single model that is trained on a large unlabelled dataset to achieve State-of-the-Art results on 11 individual NLP tasks. And all of this with little fine-tuning. That’s BERT! It’s a tectonic shift in how we design NLP models.\n",
    "\n",
    "BERT has inspired many recent NLP architectures, training approaches and language models, such as Google’s TransformerXL, OpenAI’s GPT-2, XLNet, ERNIE2.0, RoBERTa, etc.\n",
    "\n",
    "I aim to give you a comprehensive guide to not only BERT but also what impact it has had and how this is going to affect the future of NLP research. And yes, there’s a lot of Python code to work on, too!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec13d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb3113ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2b16ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e30c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "129\n",
      "197\n",
      "129\n",
      "115\n",
      "85\n",
      "178\n",
      "62\n",
      "167\n",
      "147\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35bd2d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Picture this – you’re working on a really cool data science project and have applied the latest state-of-the-art library to get a pretty good result',\n",
       " 'And boom! A few days later, there’s a new state-of-the-art framework in town that has the potential to further improve your model',\n",
       " 'That is not a hypothetical scenario – it’s the reality (and thrill) of working in the field of Natural Language Processing (NLP)! The last two years have been mind-blowing in terms of breakthroughs',\n",
       " 'I get to grips with one framework and another one, potentially even better, comes along.\\n\\nGoogle’s BERT is one such NLP framework',\n",
       " 'I’d stick my neck out and say it’s perhaps the most influential one in recent times (and we’ll see why pretty soon)',\n",
       " 'It’s not an exaggeration to say that BERT has significantly altered the NLP landscape',\n",
       " 'Imagine using a single model that is trained on a large unlabelled dataset to achieve State-of-the-Art results on 11 individual NLP tasks. And all of this with little fine-tuning',\n",
       " 'That’s BERT! It’s a tectonic shift in how we design NLP models',\n",
       " 'BERT has inspired many recent NLP architectures, training approaches and language models, such as Google’s TransformerXL, OpenAI’s GPT-2, XLNet, ERNIE2.0, RoBERTa, etc',\n",
       " 'I aim to give you a comprehensive guide to not only BERT but also what impact it has had and how this is going to affect the future of NLP research',\n",
       " 'And yes, there’s a lot of Python code to work on, too!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d232f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad7d6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = r_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "217f8187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f08e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "132\n",
      "197\n",
      "90\n",
      "157\n",
      "85\n",
      "180\n",
      "65\n",
      "168\n",
      "147\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1936b54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Picture this – you’re working on a really cool data science project and have applied the latest state-of-the-art library to get a pretty good result',\n",
       " '. And boom! A few days later, there’s a new state-of-the-art framework in town that has the potential to further improve your model.',\n",
       " 'That is not a hypothetical scenario – it’s the reality (and thrill) of working in the field of Natural Language Processing (NLP)! The last two years have been mind-blowing in terms of breakthroughs',\n",
       " '. I get to grips with one framework and another one, potentially even better, comes along.',\n",
       " 'Google’s BERT is one such NLP framework. I’d stick my neck out and say it’s perhaps the most influential one in recent times (and we’ll see why pretty soon).',\n",
       " 'It’s not an exaggeration to say that BERT has significantly altered the NLP landscape',\n",
       " '. Imagine using a single model that is trained on a large unlabelled dataset to achieve State-of-the-Art results on 11 individual NLP tasks. And all of this with little fine-tuning',\n",
       " '. That’s BERT! It’s a tectonic shift in how we design NLP models.',\n",
       " 'BERT has inspired many recent NLP architectures, training approaches and language models, such as Google’s TransformerXL, OpenAI’s GPT-2, XLNet, ERNIE2.0, RoBERTa, etc.',\n",
       " 'I aim to give you a comprehensive guide to not only BERT but also what impact it has had and how this is going to affect the future of NLP research',\n",
       " '. And yes, there’s a lot of Python code to work on, too!']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
